{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-20T20:23:15.620455Z",
     "start_time": "2024-02-20T20:23:15.590450Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, regexp_replace\n",
    "from pyspark.sql.functions import col, split, explode, regexp_replace\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "import os\n",
    "from pyspark.sql.functions import col, when, lit, isnan\n",
    "from pyspark.sql.functions import lit, nanvl, isnan, when, avg\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import CatBoostEncoder\n",
    "import pickle\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output='pandas')\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql.functions import udf\n",
    "from ast import literal_eval\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "import os\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pyspark\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "\n",
    "\n",
    "ram = 16\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('spark_first_run') \\\n",
    "    .config(\"spark.executor.memory\", f\"{ram}g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", f\"{ram}g\") \\\n",
    "    .config(\"spark.driver.memory\", f\"{ram}g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", f\"{ram}g\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "\n",
    "#    .config(\"spark.executor.memoryOverhead\", \"10g\") \\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Register Sedona functions to Spark\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import t\n",
    "\n",
    "from pyspark.sql.functions import udf, lit\n",
    "\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, FloatType\n",
    "from pyspark.sql.functions import desc\n",
    "from time import sleep\n",
    "from pyspark.sql import Row\n",
    "#from pyspark.sql.functions import min, max\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import split, explode, lower, trim\n",
    "from pyspark.sql.functions import when, count, col\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat, col, lit, cast\n",
    "from pyspark.sql.functions import collect_list, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import initcap\n",
    "import re\n",
    "import subprocess\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задача 1\n",
    "\n",
    "Выгрузить всех клиентов с суммарным весом покупок более 2 кг за последний месяц. \n",
    "Tabl A: Purchases\n",
    "client_id | date (в формате yyy-mm-dd) | sku (id товара) | cnt (количество штук)\n",
    "Tabl B: products\n",
    "sku (id товара) | weight (вес к кг)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edb83ed8d8fffeca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|client_id|total_weight|\n",
      "+---------+------------+\n",
      "|        3|         4.0|\n",
      "|        2|         5.0|\n",
      "+---------+------------+\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "data_purchases = [\n",
    "    (1, \"2024-01-05\", 101, 2),\n",
    "    (1, \"2024-01-10\", 102, 3),\n",
    "    (2, \"2024-01-15\", 103, 1),\n",
    "    (2, \"2024-02-01\", 101, 10),  \n",
    "    (3, \"2024-02-01\", 102, 5) \n",
    "]\n",
    "\n",
    "data_products = [\n",
    "    (101, 0.5),\n",
    "    (102, 0.8),\n",
    "    (103, 1.2)\n",
    "]\n",
    "\n",
    "# Создаем схему для данных\n",
    "schema_purchases = StructType([\n",
    "    StructField(\"client_id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"sku\", IntegerType(), True),\n",
    "    StructField(\"cnt\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "schema_products = StructType([\n",
    "    StructField(\"sku\", IntegerType(), True),\n",
    "    StructField(\"weight\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Создаем DataFrame из данных и применяем схему\n",
    "df_purchases = spark.createDataFrame(data_purchases, schema=schema_purchases)\n",
    "df_products = spark.createDataFrame(data_products, schema=schema_products)\n",
    "\n",
    "# Преобразуем столбец с датой в формат DateType\n",
    "df_purchases = df_purchases.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "# Определяем дату начала последнего месяца\n",
    "start_date = \"2024-02-01\"  # начало текущего месяца\n",
    "start_date_str = start_date\n",
    "\n",
    "# Вычисляем суммарный вес покупок за последний месяц\n",
    "df_total_weight = df_purchases.join(df_products, df_purchases[\"sku\"] == df_products[\"sku\"]) \\\n",
    "    .filter(col(\"date\") >= start_date_str) \\\n",
    "    .groupBy(\"client_id\") \\\n",
    "    .agg(sum(col(\"weight\") * col(\"cnt\")).alias(\"total_weight\"))\n",
    "\n",
    "# Фильтруем только те записи, у которых суммарный вес больше 2 кг\n",
    "df_result = df_total_weight.filter(col(\"total_weight\") > 2)\n",
    "\n",
    "# Выводим результат\n",
    "df_result.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T20:45:14.753043Z",
     "start_time": "2024-02-20T20:44:54.349134Z"
    }
   },
   "id": "c0fcbf80333c69d0",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задача 2\n",
    "Дана таблица MONTHLY_REVENUE - выручка за месяц по абонентам по видам услуг:\n",
    "abon_id - ID абонента\n",
    "service_id - вид услуги (смс, звонки, доп сервисы)\n",
    "revenue - выручка\n",
    "Отфильтруйте таблицу, оставив для каждого абонента только 3 услуги с максимальной выручкой.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e341ffdf27f65f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+\n",
      "|abon_id|service_id|revenue|\n",
      "+-------+----------+-------+\n",
      "|      1|       104|    200|\n",
      "|      1|       102|    150|\n",
      "|      1|       101|    100|\n",
      "|      2|       103|    140|\n",
      "|      2|       102|    130|\n",
      "|      2|       101|    120|\n",
      "|      3|       104|    120|\n",
      "|      3|       102|    100|\n",
      "|      3|       101|     90|\n",
      "+-------+----------+-------+\n"
     ]
    }
   ],
   "source": [
    "# Создаем тестовые данные\n",
    "data = [\n",
    "    (1, 101, 100),  # абонент 1, услуга 101, выручка 100\n",
    "    (1, 102, 150),\n",
    "    (1, 103, 80),\n",
    "    (1, 104, 200),\n",
    "    (2, 101, 120),\n",
    "    (2, 102, 130),\n",
    "    (2, 103, 140),\n",
    "    (2, 104, 110),\n",
    "    (3, 101, 90),\n",
    "    (3, 102, 100),\n",
    "    (3, 103, 80),\n",
    "    (3, 104, 120)\n",
    "]\n",
    "\n",
    "# Создаем DataFrame\n",
    "schema = [\"abon_id\", \"service_id\", \"revenue\"]\n",
    "MONTHLY_REVENUE = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Регистрируем таблицу во временном представлении\n",
    "MONTHLY_REVENUE.createOrReplaceTempView(\"MONTHLY_REVENUE\")\n",
    "\n",
    "# SQL Select query\n",
    "spark.sql('''\n",
    "        SELECT abon_id, service_id, revenue\n",
    "        FROM (\n",
    "            SELECT abon_id, service_id, revenue,\n",
    "                   ROW_NUMBER() OVER (PARTITION BY abon_id ORDER BY revenue DESC) as rn\n",
    "            FROM MONTHLY_REVENUE\n",
    "            ) ranked\n",
    "        WHERE rn <= 3;\n",
    "''') \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T20:53:19.602965Z",
     "start_time": "2024-02-20T20:53:08.408942Z"
    }
   },
   "id": "efd081a135de506d",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задача 3\n",
    "Найти тип первой и тип последней операции для каждого клиента и вывести их с их датами.\n",
    "В данном примере это «Оплата штрафов» от 2019-12-31 и «Оплата штрафов» от 2020-03-02.\n",
    "Желательно использовать один запрос без подзапросов с двумя единственными оконными функциями – по одной на максимум и минимум.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14ef414ac142abf4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CUSTOMER_RK: string (nullable = true)\n",
      " |-- OPERATION_DTTM: string (nullable = true)\n",
      " |-- OPERATION_TYPE_CD: string (nullable = true)\n",
      "\n",
      "+-----------+-------------------+--------------------+\n",
      "|CUSTOMER_RK|     OPERATION_DTTM|   OPERATION_TYPE_CD|\n",
      "+-----------+-------------------+--------------------+\n",
      "|       2001|2019-12-31 12:12:01|      ������ �������|\n",
      "|       2001|2020-01-10 20:48:58|                NULL|\n",
      "|       2001|2020-01-13 09:14:05|������� ������� �...|\n",
      "|       2001|2020-01-20 17:43:51|   ������ ����������|\n",
      "|       2001|2020-01-29 01:26:45|   ������ ����������|\n",
      "|       2001|2020-01-31 12:02:31|������� ������� �...|\n",
      "|       2001|2020-02-09 05:32:52|������� ������� �...|\n",
      "|       2001|2020-02-13 08:37:38|�������� ����� ��...|\n",
      "|       2001|2020-02-15 14:39:56|������� ������� �...|\n",
      "|       2001|2020-02-24 12:42:16|   ������ ����������|\n",
      "|       2001|2020-03-02 16:59:59|      ������ �������|\n",
      "+-----------+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data_ex1.csv\", sep=\";\", encoding=\"UTF-8\", header=True)\n",
    "# Показываем схему данных\n",
    "df.printSchema()\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T22:17:52.248565Z",
     "start_time": "2024-02-20T22:17:52.028569Z"
    }
   },
   "id": "b4243c158497681b",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   CUSTOMER_RK       OPERATION_DTTM              OPERATION_TYPE_CD\n0         2001  2019-12-31 12:12:01                 ������ �������\n1         2001  2020-01-10 20:48:58                           NULL\n2         2001  2020-01-13 09:14:05        ������� ������� � �����\n3         2001  2020-01-20 17:43:51              ������ ����������\n4         2001  2020-01-29 01:26:45              ������ ����������\n5         2001  2020-01-31 12:02:31        ������� ������� � �����\n6         2001  2020-02-09 05:32:52        ������� ������� � �����\n7         2001  2020-02-13 08:37:38  �������� ����� ������ �������\n8         2001  2020-02-15 14:39:56        ������� ������� � �����\n9         2001  2020-02-24 12:42:16              ������ ����������\n10        2001  2020-03-02 16:59:59                 ������ �������",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CUSTOMER_RK</th>\n      <th>OPERATION_DTTM</th>\n      <th>OPERATION_TYPE_CD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2001</td>\n      <td>2019-12-31 12:12:01</td>\n      <td>������ �������</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2001</td>\n      <td>2020-01-10 20:48:58</td>\n      <td>NULL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2001</td>\n      <td>2020-01-13 09:14:05</td>\n      <td>������� ������� � �����</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2001</td>\n      <td>2020-01-20 17:43:51</td>\n      <td>������ ����������</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2001</td>\n      <td>2020-01-29 01:26:45</td>\n      <td>������ ����������</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2001</td>\n      <td>2020-01-31 12:02:31</td>\n      <td>������� ������� � �����</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2001</td>\n      <td>2020-02-09 05:32:52</td>\n      <td>������� ������� � �����</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2001</td>\n      <td>2020-02-13 08:37:38</td>\n      <td>�������� ����� ������ �������</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2001</td>\n      <td>2020-02-15 14:39:56</td>\n      <td>������� ������� � �����</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2001</td>\n      <td>2020-02-24 12:42:16</td>\n      <td>������ ����������</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2001</td>\n      <td>2020-03-02 16:59:59</td>\n      <td>������ �������</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = df.toPandas()\n",
    "df_pandas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T22:20:22.150164Z",
     "start_time": "2024-02-20T22:20:22.086165Z"
    }
   },
   "id": "b14aae730e7e8490",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7b7b952c96ef7c83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
