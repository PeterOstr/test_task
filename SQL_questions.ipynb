{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-21T19:55:25.954389Z",
     "start_time": "2024-02-21T19:55:25.934391Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, regexp_replace\n",
    "from pyspark.sql.functions import col, split, explode, regexp_replace\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "import os\n",
    "from pyspark.sql.functions import col, when, lit, isnan\n",
    "from pyspark.sql.functions import lit, nanvl, isnan, when, avg\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import CatBoostEncoder\n",
    "import pickle\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType, ArrayType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "sklearn.set_config(transform_output='pandas')\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.sql.functions import udf\n",
    "from ast import literal_eval\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "import os\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "import pyspark\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "\n",
    "\n",
    "ram = 16\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('spark_first_run') \\\n",
    "    .config(\"spark.executor.memory\", f\"{ram}g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", f\"{ram}g\") \\\n",
    "    .config(\"spark.driver.memory\", f\"{ram}g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", f\"{ram}g\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "\n",
    "#    .config(\"spark.executor.memoryOverhead\", \"10g\") \\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Register Sedona functions to Spark\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import t\n",
    "\n",
    "from pyspark.sql.functions import udf, lit\n",
    "\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, FloatType\n",
    "from pyspark.sql.functions import desc\n",
    "from time import sleep\n",
    "from pyspark.sql import Row\n",
    "#from pyspark.sql.functions import min, max\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import split, explode, lower, trim\n",
    "from pyspark.sql.functions import when, count, col\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat, col, lit, cast\n",
    "from pyspark.sql.functions import collect_list, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import initcap\n",
    "import re\n",
    "import subprocess\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задача 1\n",
    "\n",
    "Выгрузить всех клиентов с суммарным весом покупок более 2 кг за последний месяц. \n",
    "Tabl A: Purchases\n",
    "client_id | date (в формате yyy-mm-dd) | sku (id товара) | cnt (количество штук)\n",
    "Tabl B: products\n",
    "sku (id товара) | weight (вес к кг)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edb83ed8d8fffeca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|client_id|total_weight|\n",
      "+---------+------------+\n",
      "|        3|         4.0|\n",
      "|        2|         5.0|\n",
      "+---------+------------+\n"
     ]
    }
   ],
   "source": [
    "# Пример данных\n",
    "data_purchases = [\n",
    "    (1, \"2024-01-05\", 101, 2),\n",
    "    (1, \"2024-01-10\", 102, 3),\n",
    "    (2, \"2024-01-15\", 103, 1),\n",
    "    (2, \"2024-02-01\", 101, 10),  \n",
    "    (3, \"2024-02-01\", 102, 5) \n",
    "]\n",
    "\n",
    "data_products = [\n",
    "    (101, 0.5),\n",
    "    (102, 0.8),\n",
    "    (103, 1.2)\n",
    "]\n",
    "\n",
    "# Создаем схему для данных\n",
    "schema_purchases = StructType([\n",
    "    StructField(\"client_id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"sku\", IntegerType(), True),\n",
    "    StructField(\"cnt\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "schema_products = StructType([\n",
    "    StructField(\"sku\", IntegerType(), True),\n",
    "    StructField(\"weight\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Создаем DataFrame из данных и применяем схему\n",
    "df_purchases = spark.createDataFrame(data_purchases, schema=schema_purchases)\n",
    "df_products = spark.createDataFrame(data_products, schema=schema_products)\n",
    "\n",
    "# Преобразуем столбец с датой в формат DateType\n",
    "df_purchases = df_purchases.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "# Определяем дату начала последнего месяца\n",
    "start_date = \"2024-02-01\"  # начало текущего месяца\n",
    "start_date_str = start_date\n",
    "\n",
    "# Вычисляем суммарный вес покупок за последний месяц\n",
    "df_total_weight = df_purchases.join(df_products, df_purchases[\"sku\"] == df_products[\"sku\"]) \\\n",
    "    .filter(col(\"date\") >= start_date_str) \\\n",
    "    .groupBy(\"client_id\") \\\n",
    "    .agg(sum(col(\"weight\") * col(\"cnt\")).alias(\"total_weight\"))\n",
    "\n",
    "# Фильтруем только те записи, у которых суммарный вес больше 2 кг\n",
    "df_result = df_total_weight.filter(col(\"total_weight\") > 2)\n",
    "\n",
    "# Выводим результат\n",
    "df_result.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T19:53:27.878005Z",
     "start_time": "2024-02-21T19:53:05.215054Z"
    }
   },
   "id": "c0fcbf80333c69d0",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задача 2\n",
    "Дана таблица MONTHLY_REVENUE - выручка за месяц по абонентам по видам услуг:\n",
    "abon_id - ID абонента\n",
    "service_id - вид услуги (смс, звонки, доп сервисы)\n",
    "revenue - выручка\n",
    "Отфильтруйте таблицу, оставив для каждого абонента только 3 услуги с максимальной выручкой.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95e341ffdf27f65f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+\n",
      "|abon_id|service_id|revenue|\n",
      "+-------+----------+-------+\n",
      "|      1|       104|    200|\n",
      "|      1|       102|    150|\n",
      "|      1|       101|    100|\n",
      "|      2|       103|    140|\n",
      "|      2|       102|    130|\n",
      "|      2|       101|    120|\n",
      "|      3|       104|    120|\n",
      "|      3|       102|    100|\n",
      "|      3|       101|     90|\n",
      "+-------+----------+-------+\n"
     ]
    }
   ],
   "source": [
    "# Создаем тестовые данные\n",
    "data = [\n",
    "    (1, 101, 100),  # абонент 1, услуга 101, выручка 100\n",
    "    (1, 102, 150),\n",
    "    (1, 103, 80),\n",
    "    (1, 104, 200),\n",
    "    (2, 101, 120),\n",
    "    (2, 102, 130),\n",
    "    (2, 103, 140),\n",
    "    (2, 104, 110),\n",
    "    (3, 101, 90),\n",
    "    (3, 102, 100),\n",
    "    (3, 103, 80),\n",
    "    (3, 104, 120)\n",
    "]\n",
    "\n",
    "# Создаем DataFrame\n",
    "schema = [\"abon_id\", \"service_id\", \"revenue\"]\n",
    "MONTHLY_REVENUE = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Регистрируем таблицу во временном представлении\n",
    "MONTHLY_REVENUE.createOrReplaceTempView(\"MONTHLY_REVENUE\")\n",
    "\n",
    "# SQL Select query\n",
    "spark.sql('''\n",
    "        SELECT abon_id, service_id, revenue\n",
    "        FROM (\n",
    "            SELECT abon_id, service_id, revenue,\n",
    "                   ROW_NUMBER() OVER (PARTITION BY abon_id ORDER BY revenue DESC) as rn\n",
    "            FROM MONTHLY_REVENUE\n",
    "            ) ranked\n",
    "        WHERE rn <= 3;\n",
    "''') \\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T19:53:37.109741Z",
     "start_time": "2024-02-21T19:53:27.880006Z"
    }
   },
   "id": "efd081a135de506d",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задача 3\n",
    "Найти тип первой и тип последней операции для каждого клиента и вывести их с их датами.\n",
    "В данном примере это «Оплата штрафов» от 2019-12-31 и «Оплата штрафов» от 2020-03-02.\n",
    "Желательно использовать один запрос без подзапросов с двумя единственными оконными функциями – по одной на максимум и минимум.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14ef414ac142abf4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CUSTOMER_RK: string (nullable = true)\n",
      " |-- OPERATION_DTTM: string (nullable = true)\n",
      " |-- OPERATION_TYPE_CD: string (nullable = true)\n",
      "\n",
      "+-----------+-------------------+--------------------+\n",
      "|CUSTOMER_RK|     OPERATION_DTTM|   OPERATION_TYPE_CD|\n",
      "+-----------+-------------------+--------------------+\n",
      "|       2001|2019-12-31 12:12:01|      Оплата штрафов|\n",
      "|       2001|2020-01-10 20:48:58|                NULL|\n",
      "|       2001|2020-01-13 09:14:05|Покупка товаров и...|\n",
      "|       2001|2020-01-20 17:43:51|   Оплата мобильного|\n",
      "|       2001|2020-01-29 01:26:45|   Оплата мобильного|\n",
      "|       2001|2020-01-31 12:02:31|Покупка товаров и...|\n",
      "|       2001|2020-02-09 05:32:52|Покупка товаров и...|\n",
      "|       2001|2020-02-13 08:37:38|Переводы между св...|\n",
      "|       2001|2020-02-15 14:39:56|Покупка товаров и...|\n",
      "|       2001|2020-02-24 12:42:16|   Оплата мобильного|\n",
      "|       2001|2020-03-02 16:59:59|      Оплата штрафов|\n",
      "+-----------+-------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data_ex1.csv\", sep=\";\", encoding=\"UTF-8\", header=True)\n",
    "# Показываем схему данных\n",
    "df.printSchema()\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T19:59:02.936754Z",
     "start_time": "2024-02-21T19:59:02.658478Z"
    }
   },
   "id": "b4243c158497681b",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# df_pandas = df.toPandas()\n",
    "# df_pandas\n",
    "schema = [\"abon_id\", \"OPERATION_DTTM\", \"OPERATION_TYPE_CD\"]\n",
    "\n",
    "# Создаем временное представление для DataFrame\n",
    "df.createOrReplaceTempView(\"operations\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:01:37.147225Z",
     "start_time": "2024-02-21T20:01:37.123225Z"
    }
   },
   "id": "b14aae730e7e8490",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-------------------+-------------------+\n",
      "|CUSTOMER_RK|FIRST_OPERATION_TYPE|FIRST_OPERATION_DATE|LAST_OPERATION_TYPE|LAST_OPERATION_DATE|\n",
      "+-----------+--------------------+--------------------+-------------------+-------------------+\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "|       2001|      Оплата штрафов| 2019-12-31 12:12:01|     Оплата штрафов|2020-03-02 16:59:59|\n",
      "+-----------+--------------------+--------------------+-------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# SQL Select query\n",
    "spark.sql('''\n",
    "    SELECT\n",
    "        CUSTOMER_RK,\n",
    "        FIRST_VALUE(OPERATION_TYPE_CD) OVER (PARTITION BY CUSTOMER_RK ORDER BY OPERATION_DTTM) AS FIRST_OPERATION_TYPE,\n",
    "        FIRST_VALUE(OPERATION_DTTM) OVER (PARTITION BY CUSTOMER_RK ORDER BY OPERATION_DTTM) AS FIRST_OPERATION_DATE,\n",
    "        LAST_VALUE(OPERATION_TYPE_CD) OVER (PARTITION BY CUSTOMER_RK ORDER BY OPERATION_DTTM ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS LAST_OPERATION_TYPE,\n",
    "        LAST_VALUE(OPERATION_DTTM) OVER (PARTITION BY CUSTOMER_RK ORDER BY OPERATION_DTTM ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS LAST_OPERATION_DATE\n",
    "    FROM\n",
    "        operations;\n",
    "''') \\\n",
    "    .show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:02:41.674622Z",
     "start_time": "2024-02-21T20:02:41.411210Z"
    }
   },
   "id": "7b7b952c96ef7c83",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Задача 4\n",
    "Для таблицы из п.3 найти для каждой операции количество дней с даты последней оплаты мобильного (операция \"Оплата мобильного\"). Если тип операции «Оплата мобильного», то кол-во дней равно 0.\n",
    "Т.е. для 2020-02-15 последняя операция «Оплата мобильного» была 2020-01-29, т.е. кол-во дней составит 17 дней\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "957cf44f63f2e52"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------------------+\n",
      "|     OPERATION_DTTM|   OPERATION_TYPE_CD|DAYS_SINCE_LAST_MOBILE_PAYMENT|\n",
      "+-------------------+--------------------+------------------------------+\n",
      "|2019-12-31 12:12:01|      Оплата штрафов|                            55|\n",
      "|2020-01-10 20:48:58|                NULL|                            45|\n",
      "|2020-01-13 09:14:05|Покупка товаров и...|                            42|\n",
      "|2020-01-20 17:43:51|   Оплата мобильного|                             0|\n",
      "|2020-01-29 01:26:45|   Оплата мобильного|                             0|\n",
      "|2020-01-31 12:02:31|Покупка товаров и...|                            24|\n",
      "|2020-02-09 05:32:52|Покупка товаров и...|                            15|\n",
      "|2020-02-13 08:37:38|Переводы между св...|                            11|\n",
      "|2020-02-15 14:39:56|Покупка товаров и...|                             9|\n",
      "|2020-02-24 12:42:16|   Оплата мобильного|                             0|\n",
      "|2020-03-02 16:59:59|      Оплата штрафов|                            -7|\n",
      "+-------------------+--------------------+------------------------------+\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql('''\n",
    "    SELECT\n",
    "        OPERATION_DTTM,\n",
    "        OPERATION_TYPE_CD,\n",
    "        CASE\n",
    "            WHEN OPERATION_TYPE_CD = 'Оплата мобильного' THEN 0\n",
    "            ELSE datediff(max(CASE WHEN OPERATION_TYPE_CD = 'Оплата мобильного' THEN OPERATION_DTTM END) OVER (), OPERATION_DTTM)\n",
    "        END AS DAYS_SINCE_LAST_MOBILE_PAYMENT\n",
    "    FROM\n",
    "        operations;\n",
    "''')\n",
    "\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T20:04:30.583861Z",
     "start_time": "2024-02-21T20:04:30.289562Z"
    }
   },
   "id": "72c59076de9eed4c",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "96e5282091791917"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a809c7f8532821ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
